"""
main.py

This script sets up a FastAPI web server that exposes a /generate endpoint.
It receives a user message via a POST request and returns a chatbot-style response
generated by TinyLlama, using the logic defined in openai_chat.py.
"""

# Import FastAPI to create the web API
from fastapi import FastAPI

# Import BaseModel from Pydantic to define the structure of incoming request data
from pydantic import BaseModel

# Import the chatbot response function from your service module
from sana_ai.services.openai_chat import get_chat_response

# Create an instance of the FastAPI application
app = FastAPI()


# Define the expected structure of the incoming JSON payload
class Prompt(BaseModel):
    message: str  # The user's message to send to the chatbot


# Define a POST endpoint at /generate that accepts a Prompt object
@app.post("/generate")
async def generate_text(prompt: Prompt) -> dict[str, str]:
    """
    Receives a user message and returns a chatbot-generated response.

    Args:
        prompt (Prompt): A Pydantic model containing the user's message.

    Returns:
        dict[str, str]: A dictionary with the generated response.
    """
    # Call the chatbot function with the user's message
    response = get_chat_response(prompt.message)

    # Return the response in JSON format
    return {"response": response}
